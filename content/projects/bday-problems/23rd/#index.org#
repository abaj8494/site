+++
title = "23 Birthday Problems"
categories = ["bday-problems"]
tags = ["matrix-calculus", "power-series", "probability", "mle", "least-squares", "regression"]
math = "true"
+++

#+begin_export html
<aside class="custom-toc">
<nav>
<p class="sidebar__heading">Table Of Contents</p>
#+end_export

#+toc: headlines 2

#+begin_export html
</nav></aside>
#+end_export

{{< collapse folded="false" >}}

* Solutions

** Q1

*** a)
We use the update rule \[x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)}\]

To produce \(x_1 = 1.5\) and \(x_2 = 1.416\). Note to 3 decimal places \(\sqrt{2} \approx 1.412\).

*** b)

We can adjust our update rule to predict the roots of the derivative function[fn:1], i.e. it's local optima:
\[x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}\]

*** c)

Advantage: quadratic convergence.

Disadvantage: computing a second derivative may be difficult, think large and sparse matrices. Furthermore, an analytic approach may not exist.

** Q2

*** a)

Let "SB" denote the phrase "share birthdays":
\[\begin{align*}
\mathcal{P}(\geq 2~\mathrm{SB}) &= \mathcal{P}(3~\mathrm{SB}) + \mathcal{P}(4~\mathrm{SB}) + \ldots + \mathcal{P}(N~\mathrm{SB})\\
&= 1 - \mathcal{P}(\text{nobody }\mathrm{SB})\\
\mathcal{P}(\text{nobody }\mathrm{SB}) &= \frac{365}{365}\times\frac{364}{365}\times\ldots\frac{365-n+1}{365}\\
&= \frac{365!}{365^n(365-n)!}
\end{align*}\]

However, there is no obvious closed form value of \(n\) which satisfies \(\frac{365!}{365^n(365-n)! < \frac{1}{2}\).

We can massage the equivalent LHS expression \(\frac{365}{365}\times\frac{364}{365}\times\ldots\frac{365-n+1}{365}\) into
\[\prod_{k=0}^{n-1}\frac{365-k}{365}\]

which can then be expressed in code:

#+BEGIN_SRC jupyter-python :session 23bp
  import math

  def no_shared_birthday_prob(n):
    probability = 1.0
    for k in range(n):
      probability *= (365 - k) / 365
    return probability

  threshold = 1/2
  n = 1

  while no_shared_birthday_prob(n) >= threshold:
    n += 1

  print(f"thus the smallest n such that p(no_shared_birthday) <= 1/2 = {n}")

#+END_SRC

#+RESULTS:
: thus the smallest n such that p(no_shared_birthday) <= 1/2 = 23

*** b)

By the pigeonhole principle, it would be 366 people for a regular year, and 367 for a leap year.

** Q3

*** a)

This matrix is non-square and thus does not possess an inverse.

*** b)

\[\begin{align*}
\begin{bmatrix}0&2&4\\2&4&6\\4&6&8\end{bmatrix} &= 0\begin{vmatrix}4&6\\6&8\end{vmatrix} - 2\begin{vmatrix}2&6\\4&8\end{vmatrix} + 4\begin{vmatrix}2&4\\4&6\end{vmatrix} \\
&= -2(16-24) + 4(12-16)\\
&= 16 - 16 \\
&= 0
\end{align*}\]

Thus this matrix does not have an inverse.

*** c)

Since the determinant of a triangular matrix is equal to the product of the diagonal elements, we have a non-zero determinant and can perform the row-reduction on \(A|I\).

#+BEGIN_SRC jupyter-python :session 23bp :results none
  from sympy import *

  m = Matrix([
    [4, 3, 2, 1, 1, 0, 0, 0],
    [0, 4, 3, 2, 0, 1, 0, 0],
    [0, 0, 4, 3, 0, 0, 1, 0],
    [0, 0, 0, 4, 0, 0, 0, 1]])
  print(m.rref())
#+END_SRC

\[ I|A^{-1} = \begin{bmatrix}\cfrac{1}{4}&-\cfrac{3}{16}&\cfrac{1}{64}&\cfrac{5}{256}\\0&\cfrac{1}{4}&-\cfrac{3}{16}&\cfrac{1}{64}\\0&0&\cfrac{1}{4}&-\cfrac{3}{16}\\0&0&0&\cfrac{1}{4}\end{bmatrix}\]

** Q4

*** a)
\[3\]

*** b)
\[6\]

** Q5

*** a)
The trace of a matrix is equal to the *sum* of the eigenvalues.

*** b)
The determinant of a matrix is equal to the *product* of its eigenvalues.

*** c)
\[\lambda_1 = -4, \lambda_2 = 6\]

*** d)
Eigendecompositions do not always exist, but Singular Value Decompositions do.

** Q6

\[\begin{align*}
A &= zz^T\\
A^T&=(zz^T)^T\\
&=zz^T\\
&=A\end{align*}\]

Furthermore,
\[\begin{align*}
\forall x\in \mathbb{R}^n,~ x^Tzz^Tx &= x^Tz^Tzx\\
&=\|zx\|^2_2\\
&\geq 0\qquad\quad\text{by definition of the L2 norm.}\end{align*}\]

** Q7

*** a)
Separating and integrating:
\[\begin{align*}
\int\frac{\mathrm{d}y}{1-y} &= \int \cos(x) \mathrm{d}x\\
\implies\displaystyle y &= 1 - 2e^{\displaystyle 1-\sin(x)}\end{align*}\]

*** b)
Solving for the particular and homogenous solutions and combining them:
\[\large y = \underbrace{Ae^{-7t} + Bte^{-7t}}_{y_h} + \underbrace{\frac{1}{64}e^t}_{y_p}\]

** Q8

*** a)

\[\sum_{n=0}^\infty \frac{z^n}{z!} = z + \frac{z^2}{2!} + \frac{z^3}{3!} + \ldots \]

*** b)

\[\sin(x) = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \ldots\]

*** c)

\[\cos(x) = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \ldots\]

*** d)

\[\sinh(x) = x + \frac{x^3}{3!} + \frac{x^5}{5!} + \frac{x^7}{7!} + \ldots \]

*** e)

\[\cosh(x) = 1 + \frac{x^2}{2!} + \frac{x^4}{4!} + \frac{x^6}{6!} + \ldots\]

*** f)
\[\frac{1}{1-x} = 1 + x + x^2 + x^3 + \ldots\]

*** g)
\[\frac{1}{1-x} = 1 - x + x^2 - x^3 + \ldots\]

** Q9

\[large a_n = \frac{2^{(n+1)}}{(n+2)!}\]

** Q10

Perform a couple of row reductions to make the matrix as triangular as possible:
- L1 = L1
- L2 = L2 - L1
- L3 = L3 - L2 + L1
- L4 = L4 + L1
- L5 = L5 + L4

\[\begin{vmatrix}2&0&1&2&0\\2&-1&0&1&1\\0&1&2&1&2\\-2&0&2&-1&2\\2&0&0&1&1\end{vmatrix}
\leadsto\begin{vmatrix}
    2&0&1&2&0\\
    0&-1&-1&-1&1\\
    0&0&1&0&3\\
    0&0&3&1&2\\
    0&0&2&0&3
\end{vmatrix}\]

Then Laplace Expansion across the rows leads us to discover that the determinant is just:
\[2\times -1\times \begin{vmatrix}1&0&3\\3&1&2\\2&0&3\end{vmatrix} = 6\]

** Q11

\[\large\begin{align*}
\sigma (z) &= \frac{1}{1+e^{-z}}\\
\sigma '(z) &= (1+e^{-z})^{-2} e^{-z}\\
&= \frac{e^{-z}}{(1+e^{-z})^2}\\
&= \frac{1}{1+e^{-z}} \times \frac{1+e^{-z}-1}{1+e^{-z}}\\
\sigma '(z) &= \sigma(z)(1-\sigma(z))\quad \small\square\end{align*}\]

** Q12

*** a)
A single /yes/ or /no/ experiment. Something such as a coin toss.

\[P(X = x) = \begin{cases} 
p & \text{if } x = 1 \\
1 - p & \text{if } x = 0\end{cases}\]
where \( p \) is the probability of success, and \( X \in \{0, 1\} \).

You only have a single trial, and it is either successful or not.

*** b)
A *series* of the _Bernoulli_ \(n\) independent /yes/, /no/ experiments. Something such as counting the number of heads when you flip the coin \(N\) times.

\[P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\]
where:
- \( n \): number of trials,
- \( k \): number of successes,
- \( p \): probability of success in each trial,
- \( \binom{n}{k} = \frac{n!}{k!(n-k)!} \).

*** c)

A /continuous distribution/ with a bell-shaped curve. Its PDF is governed by:

\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}\]
where:
- \( \mu \): mean (center of the distribution),
- \( \sigma^2 \): variance (spread of the distribution).

And an example experiment which is has this probability distribution is the heights of people in the world.

*** d)

This distribution models the 80/20 phenomena. An example would be distribution of wealth (obviously). It's PDF is given by:
\[f(x) = \frac{\alpha x_m^\alpha}{x^{\alpha+1}} \quad \text{for } x \geq x_m\]
where:
- \( \alpha > 0 \): shape parameter,
- \( x_m > 0 \): scale parameter (minimum value).

*** e)

The geometric distribution models the /number of trials/ required to obtain the /first/ success in a sequence of indepedent _Bernoulli_ trials. i.e. rolling a dice until you finally see a 6.

\[P(X = k) = (1-p)^{k-1}p\]
- \( p \): probability of success in each trial,
- \( k \): trial number of the first success (\( k = 1, 2, 3, \dots \)).

*** f)

This distribution models the number of events that happen in a fixed interval of time or space. i.e. the number of phone calls a service company receives in a 2 hour block.

\[P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\]
where:
- \( \lambda > 0 \): average rate of events in a fixed interval,
- \( k \): number of events (\( k = 0, 1, 2, \dots \)).

*** g)

This represents the experiments within which all outcomes are equally likely to occur. Like flipping a coin as heads[fn:2] or rolling a 6 on a dice.

We have both a discrete and continuous case:

Discrete:

\[P(X=x)=\cfrac{1}{n}\quad\text{for }x\in \{x_1, x_2,\ldots,x_n\}\]

Continuous:
\[f(x) = \begin{cases} 
\cfrac{1}{b-a} & \text{if } a \leq x \leq b \\
0 & \text{otherwise}
\end{cases}\]
where:
- \( a \): lower bound,
- \( b \): upper bound.

** Q13

\(n\choose r\) is choosing \(r\) objects from a set of \(n\) distinct objects.

Realising that \(r\) is of the same type as \(n\), i.e. if \(n\) is 5 shoes, \(r\) could be 1, 2, 3, 4 or 5 /shoes/.

Then \(n\choose r\) would grant you 5 options for the first shoe, then 4, etc. Thus we can choose \(n\times(n-1)\times\ldots(n-r+1)\) items.

However, since there were \(r\) slots for each shoe to be in, and these selections may be ordered \(r!\) ways without changing /which/ set of shoes you end up with we divide through by \(r!\).

\[{n\choose r} = \cfrac{n\times(n-1)\times\ldots\times(n-r+1)}{r!}\]
which can then be massaged by a clever trick of \(\times \cfrac{(n-r)!}{(n-r)!}\) such that we are allowed to continue the factorial of the numerator beyond \((n-r+1)\) to \(n(n-1)\times\ldots(n-r+1)(n-r)(n-r-1)\times\ldots\times 1\) which ultimately equals \(n!\).

The denominator then just accepts the multiplication to form \(r!(n-r)!\), such that our final closed form to \(n\choose r\) is

\[\frac{n!}{r!(n-r)!}\square\]

** Q14

*** a)

\[\Large\begin{align*}
\frac{\partial}{\partial{w_0}} \mathcal{L}(\vec{w}) &= \frac{\partial}{\partial{w_0}}~ \frac{1}{n}\sum(y_i-w_0-w_1x_i)^2  \\
0&\stackrel{\text{(set)}}{=} -\frac{2}{n}\sum(y_i-w_0-w_1x_i) \\
&= -2\left[ \frac{1}{n}\sum y_i - \frac{1}{n}\sum w_0 - \frac{1}{n}\sum w_1 x_i \right ] \\
&= -2 \left[ \bar{y} - w_0 - w_1 \bar{x} \right ]\\
\therefore w_0 &= \bar{y}-w_1\bar{x}\end{align*}\]

*** b)
\[\Large\begin{align*}
\frac{\partial}{\partial{w_1}} \mathcal{L}(\vec{w}) &= \frac{\partial}{\partial{w_1}}~ \frac{1}{n}\sum(y_i-w_0-w_1x_i)^2  \\
0&\stackrel{\text{(set)}}{=} -\frac{2}{n}\sum(y_i-w_0-w_1x_i)x_i\\
&= -2\left[ \frac{1}{n}\sum y_ix_i -\frac{1}{n}\sum w_0x_i -\frac{1}{n}\sum w_1x_i^2 \right ] \\
&= \overline{xy} -w_0 \overline{x} -w_1 \overline{x^2} \\
\therefore w_1 &= \frac{\overline{xy}-w_0\overline{x}}{\overline{x^2}}
\end{align*}\]

*** c)

Substitute \(w_0\) into \(w_1\):
\[\Large\begin{align*}
w_1 &= \frac{\overline{xy}-(\overline{y}-w_1\overline{x})\overline{x}}{\overline{x^2}} \\
&= \frac{\overline{xy}-\bar{x}\bar{y}+w_1\overline{x}^2}{\overline{x^2}}\\
\implies w_1\overline{x^2} - w_1\overline{x}^2 &= \overline{xy} - \bar{x}\bar{y} \\
w_1(\overline{x^2}-\overline{x}^2) &= \overline{xy} - \bar{x}\bar{y} \\
w_1 &= \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\overline{x}^2} \\
\implies w_0 &= \overline{y} - \left ( \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\overline{x}^2}\overline{x} \right ) \\
\text{and thus }\quad \hat{y}(x) &= \overline{y} - \left ( \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\overline{x}^2}\overline{x} \right ) + \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\overline{x}^2}x
\end{align*}\]

** Q15

*** a)

\(\large\text{RTS: } \nabla_{\vec{x}} \vec{b}^T \vec{x} = \vec{b}\).

\[\large\begin{align*}
\vec{b}^T \vec{x} &= 
    1\left\{\vphantom{\begin{bmatrix} b_1 & b_2 & \ldots & b_n \end{bmatrix}}\right.
    \underbrace{\begin{bmatrix} b_1 & b_2 & \ldots & b_n \end{bmatrix}}_{n}
    \overbrace{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}}^{1}
    \left.\vphantom{\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}}\right\}n\\
&= b_1 x_1 + \ldots + b_n x_n \\
\implies \frac{\partial}{\partial x_i} \vec{b}^T \vec{x} &= b_i \\
\therefore \quad \nabla_{\vec{x}} \vec{b}^T \vec{x} &= \vec{b} \, \square
\end{align*}\]

*** b)

\(\large\text{RTS: } \nabla_{\vec{x}} \vec{x}^T\vec{A}\vec{x} = 2\vec{A}\vec{x}\).

\[\large\begin{align*}
\vec{x}^T \vec{A}\vec{x} &=
    \begin{bmatrix}x_1&x_2&\ldots &x_n\end{bmatrix}
    \begin{bmatrix}A_{1,1}&\dots &A_{1,n}\\A_{2,1}&\dots &A_{2,n}\\\vdots &\ddots &\vdots\\A_{n,1}&\dots &A_{n,n}\end{bmatrix}
    \begin{bmatrix}x_1\\x_2\\\vdots \\x_n\end{bmatrix} \\
&= \begin{bmatrix}A_{1,1}x_1+A_{2,1}x_2+\ldots+A_{n,1}x_n&A_{1,2}x_1+A_{2,2}x_2+\ldots +A_{n,2}x_n&A_{1,n}x_1 +\ldots A_{n,n}x_n\end{bmatrix}
    \begin{bmatrix}x_1\\x_2\\\vdots \\x_n\end{bmatrix} \\
&= x_1[A_{1,1}x_1+A_{2,1}x_2+\ldots+A_{n,1}x_n] + x_2[A_{1,2}x_1+A_{2,2}x_2+\ldots +A_{n,2}x_n] + \ldots + x_n [A_{1,n}x_1 +\ldots A_{n,n}x_n] \\
&= [A_{1,1}x_1^2+A_{2,1}x_1x_2+\ldots+A_{n,1}x_1x_n] + [A_{1,2}x_1x_2+A_{2,2}x_2^2+\ldots +A_{n,2}x_2x_n] + [\ldots] + [A_{1,n}x_1x_n +\ldots A_{n,n}x_n^2] \\
\implies \frac{\partial}{\partial{x_i}} \nabla_{\vec{x}} \vec{x}^T\vec{A}\vec{x} &= 2A_{i,i}x_i + \sum_{j\neq i}^n A_{j,i}x_j + \sum_{j\neq i}^n A_{i,j}x_j \\
&= \sum_{j=1}^n 2 A_{j,i}x_i\quad\text{by the symmetry of A} \\
\text{Thus, }\nabla_{\vec{x}} \vec{x}^T\vec{A}\vec{x} = 2\vec{A}\vec{x}\square.
\end{align*}\]

*** c)

\[\mathcal{L}(\vec{w}) = \frac{1}{n} \|\vec{y}-\vec{X}\vec{w}\|_2^2\]


* Source Code
#+INCLUDE: "/code/latex/bday-problems/23/main.tex" src latex

* Footnotes
[fn:2] note, this is mathematically equivalent to the Bernoulli case, just that the emphasis there would be on binary nature of the outcome, whilst the Uniform distribution focusses on just that --- the uniformity of the outcomes. 

[fn:1] as opposed to the roots of the original function (x-intercepts). 
