+++
title = "Optimiser Paradigms in Machine Learning"
+++

just taking notes for the moment. a video by Sourish Kundu

you want to take less steps over all

* sgd

** vanilla
\[W_{t+1} = W_t - \alpha\nabla W_t \]

* sgd with momentum

\[V_{t+1} = \beta V_t + (1-\beta)\nabla W_t\\
W_{t+1} = W_t - \alpha V_{t+1}\\
\beta = 0.9\]

* rms prop
* adam
print the paper please

Recall that a Neural Network follows the following construction:
1. Pass data (forward) through model to get predicted values
2. Calculate loss with predicted values against labels
3. Perform backpropagation w.r.t each weight / bias to get the direction in which to move that weight such that it moves closer to the global minima
4. Update parameters with gradients using an optimiser.

* momentum

ball's pace slows down
this makes total fkn sense! if the gradient signs are the same, increasing your confidence in that direction and move further.

* environment
better optimiser, means less env fuckup
