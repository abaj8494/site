+++
title = "Optimiser Paradigms in Machine Learning"
toc = "true"
math = "true"
+++

* deep learning pipeline

Recall that a Neural Network follows the following construction:
1. Pass data (forward) through model to get predicted values
2. Calculate loss with predicted values against labels
3. Perform backpropagation w.r.t each weight / bias to get the direction in which to move that weight such that it moves closer to the global minima
4. Update parameters with gradients using an optimiser.

* momentum

ball's pace slows down
this makes total fkn sense! if the gradient signs are the same, increasing your confidence in that direction and move further.
you want to take less steps over all

* optimisers
** sgd
you want to take less steps over all

*** vanilla
\[W_{t+1} = W_t - \alpha\nabla W_t \]

*** with momentum

\[V_{t+1} = \beta V_t + (1-\beta)\nabla W_t\\
W_{t+1} = W_t - \alpha V_{t+1}\\
\beta = 0.9\]

clearly if you unroll the \(V_{t+1}\) term, you are equivalently unrolling the exponential weighted average:
\[\begin{align*}
V_{t+1} &= \beta V_t + (1-\beta)\nabla W_t\\
V_{t+2} &= \beta [\beta V_t + (1-\beta)\nabla W_t] + (1-\beta)\nabla W_t\\
&= \beta^2 V_t + \beta(1-\beta)\nabla W_t + (1-\beta)\nabla W_t\\\]

which represents all the subsequently older gradients with more decayed contributions to the current gradient; nonetheless the are contributive.

*** nag (Nesterov Accelerated Gradient)
is also an option. it adds a corrective term to sgd momentum:

from
\[V_{t+1} = \beta V_t + (1-\beta)\nabla W_t\]
to
\[V_{t+1} = \beta V_t + \alpha \nabla(W_t - \beta V_t)\]

*** algorithm

from the [[https://pytorch.org/docs/stable/generated/torch.optim.SGD.html][pytorch]] site:

\[\begin{aligned}
    &\rule{110mm}{0.4pt}  \\ 
    &\textbf{input} : \gamma \text{ (lr)}, \theta_0 \text{ (params)}, f(\theta) \text{ (objective)}, 
    \lambda \text{ (weight decay)}, \\ 
    &\hspace{13mm} \mu \text{ (momentum)}, \tau \text{ (dampening)}, \textit{ nesterov, maximize} \\[-1.ex]
    &\rule{110mm}{0.4pt}  \\ 
    &\textbf{for} \quad t=1 \quad \textbf{to} \quad \ldots \quad \textbf{do} \\ 
    &\hspace{5mm} g_t \leftarrow \nabla_{\theta} f_t (\theta_{t-1}) \\ 
    &\hspace{5mm} \textbf{if} \quad \lambda \neq 0 \\ 
    &\hspace{10mm} g_t \leftarrow g_t + \lambda \theta_{t-1} \\ 
    &\hspace{5mm} \textbf{if} \quad \mu \neq 0 \\ 
    &\hspace{10mm} \textbf{if} \quad t > 1 \\ 
    &\hspace{15mm} \mathbf{b}_t \leftarrow \mu \mathbf{b}_{t-1} + (1 - \tau) g_t \\ 
    &\hspace{10mm} \textbf{else} \\ 
    &\hspace{15mm} \mathbf{b}_t \leftarrow g_t \\ 
    &\hspace{5mm} \textbf{if} \quad \textit{nesterov} \\ 
    &\hspace{10mm} g_t \leftarrow g_t + \mu \mathbf{b}_t \\ 
    &\hspace{5mm} \textbf{if} \quad \textit{maximize} \\ 
    &\hspace{10mm} \theta_t \leftarrow \theta_{t-1} + \gamma g_t \\ 
    &\hspace{5mm} \textbf{else} \\ 
    &\hspace{10mm} \theta_t \leftarrow \theta_{t-1} - \gamma g_t \\ 
    &\rule{110mm}{0.4pt} \\ 
    &\textbf{return} \quad \theta_t
\end{aligned}\]

*** limitations

if the gradients become really small, then you'll move towards the minimum /really/ slowly.

this can happen with sparse data.

** rms prop (Root Mean-Squared Propagation)

the key feature is that this optimiser can adjust the learning rate for each parameter. it steals the idea from ada grad (Adaptive Gradients).

\[V_{t+1} = \beta V_t + (1-\beta)\nabla W_t^2\\
W_{t+1} = W_t - \alpha \frac{\nabla W_t}{\sqrt{V_{t+1}+\epsilon}}\\
\beta = 0.9\qquad\text{(compare apples with apples)}\]

*** ada grad

ada grad does not weight down older gradients.
\[V_{t+1} = V_t + \nabla W_t^2\\
W_{t+1} = W_t - \alpha \frac{\nabla W_t}{\sqrt{V_{t+1}+\epsilon}}\quad\text{(this is the same)}\]

*** ada delta

limits the number of past gradients that are included in the calculation of how to modulate the learning rate.

** adam (Adaptive Moment Estimation)

combination of momentum *and* adaptive gradients

the first moment is referred to as the mean. if the magnitude of the mean is higher that indicates the gradients are pointing in the same direction.

variance, or squared-error is the second moment. this is what rms prop uses to modulate the learning rates individually for each parameter (based on its gradients).{{< mnote "we are only calculating the uncentered variance (not substracting from the mean" >}}.
high variance -> smaller steps, variance low -> bigger steps.

\[\begin{align}
M_t &= \beta_1 M_{t-1} + (1-\beta_1)\nabla W_t \\
V_t &= \beta_2 V_{t-1} + (1-\beta_2)\nabla W_t^2 \\
\hat{M}_t &= \cfrac{M_t}{1-\beta_1^t}\\
\hat{V}_t &= \cfrac{V_t}{1-\beta_2^t}\\
W_{t+1} &= W_t -\alpha \cfrac{\hat{M}_t}{\sqrt{\hat{V}_t}+\epsilon}\\
\beta_1 &= 0.9\\
\beta_2 &= 0.99\\
\epsilon &= 10^{-8}\\
\end{align}\]

might delete:
\[\begin{align}
M_t &= \beta_1 M_{t-1} + (1-\beta_1)\nabla W_t \\
V_t &= \beta_2 V_{t-1} + (1-\beta_2)\nabla W_t^2 \\
\left.\begin{aligned}
\hat{M}_t &= \cfrac{M_t}{1-\beta_1^t}\\
\hat{V}_t &= \cfrac{V_t}{1-\beta_2^t}
\end{aligned}\right\} \text{ bias correction} \\
W_{t+1} &= W_t -\alpha \cfrac{\hat{M}_t}{\sqrt{\hat{V}_t}+\epsilon}\\
\beta_1 &= 0.9\\
\beta_2 &= 0.99\\
\epsilon &= 10^{-8}\\
\end{align}\]

* experiments

#+begin_center
[[{{ < cwd >}}cifar10][3x32x32 dimensionality per image]]
#+end_center

#+begin_src jupyter-python :session optims
  import torch
  import torchvision
  import torchvision.transforms as transforms
  
  model = torch.hub.load('/pytorch/vision:v0.10.0', 'resnet18', preTrained=True)


#+end_src

* environment (earth)
better optimiser, means less ruining earth.

* reinforcement learning

rmsprop might be better than adam because as the agent explores new areas of the environment the distribution of training data tends to shift. what used to be the best action towards the beginning of the game, may not continue to be so later on in the game.

thus, samples rl are not i.i.d. and momentum may be a detriment to performance.

* memory

for each parameter, we store 2 extra values, as such adam is the least memory efficient optimiser; tripling memory usage.

just taking notes for the moment. a video by Sourish Kundu


* references
