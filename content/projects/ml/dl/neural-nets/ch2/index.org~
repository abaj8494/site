+++
title = "chapter 2: how the backpropagation algorithm works"
mathjax = "true"
toc = "true"
+++

- the algorithm was introduced in the 1970s, but its importance wasn't fully appreciated until the famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams.
- "workhorse of learning in neural networks"
- at the heart of it is an expression that tells us how quickly the cost function changes when we change the weights and biases.

#+BEGIN_CENTER
#+ATTR_HTML: :class lateximage :width 700px :id activations
#+CAPTION: activation diagram of a single neuron in matrix notation
[[{{< cwd >}}activations.svg]]
#+END_CENTER

** notation
- \(w_{jk}^l\){{< mnote "we go from the l\(^{th}\) neuron to the \((l-1)^{th}\) as opposed to going forward because this way \ref{eq:vec_ac} can be written as \(wa\) and not \(aw^T\)" >}} means the weight of the j\(^{th}\) neuron in layer \(l\) to the k\(^{th}\) neuron in the previous layer 
  - similarly for \(b_j^l\) we say it is the j\(^{th}\) bias in the l\(^{th}\) layer
  - and \(a_j^l\) we say is the j\(^{th}\) activation in the l\(^{th}\) layer

obviously the activations of \(a_j^l\) are computed with:
\begin{equation}
\label{eq:ac_sum}
a_j^l = \sigma(\sum_k w_{jk}^l a_k^{l-1} + b_j^l)
\end{equation}

and says that the activation of any given neuron in layer l = the sigmoid of the weights multiplied by activations for all inputs of the previous layer + bias.

we can then vectorise this:
\begin{equation}
\label{eq:ac_vec}
a^l = \sigma(w^la^{l-1} + b^l)
\end{equation}

it appears that the 'juicy bit' of the sigma above is worth naming: \(w^la^{l-1} + b^l = z^l\).


** cost function
as before, we have:
\begin{equation}
\label{eq:cost}
C = \cfrac{1}{2n}\sum_x \|y(x) -a^L(x) \|^2
\end{equation}

we make two assumptions about this cost function.
1. the cost function /can/ be written as an average \(C = \frac{1}{n}\sum_x C_x\) over cost functions \(C_x\) for individual training examples, \(x\).
2. the cost function /can/ be written as a function of the outputs from the neural network.

** hadamard product

it's just the element-wise multiplication of vectors.

implementing back-prop with this operator tends to be quite fast. (faster than matrix multiplication at times).

** four fundamental equations.
\[\require{bbox}\require{color}\definecolor{myred}{RGB}{128,0,0}
\fcolorbox{red}{lightblue}{$\begin{align}
\color{myred}{\boldsymbol{\delta^L}} &\color{myred}{\boldsymbol{= \nabla_a C \odot \sigma'(z^L)}} \label{eq:bp1}\\
\color{myred}{\boldsymbol{\delta^l}} &\color{myred}{\boldsymbol{= ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)}} \label{eq:bp2}\\
\color{myred}{\boldsymbol{\cfrac{\partial C}{\partial b_j^l}}} &\color{myred}{\boldsymbol{= \delta_j^l}} \label{eq:bp3}\\
\color{myred}{\boldsymbol{\cfrac{\partial C}{\partial w_{jk}^l}}} &\color{myred}{\boldsymbol{= a_k^{l-1} \delta_j^l}} \label{eq:bp4}
\end{align}$}\]

where:
- \(\delta_j^l = \cfrac{\partial C}{\partial z_j^l}\) is the error of neuron j in layer l.
- L is the last layer
- and then \(\delta_j^L = \cfrac{\partial C}{\partial a_j^L}\sigma'(z^L_j) = \nabla_a C \odot \sigma'(z^L) = (a^L -y) \odot \sigma'(z^L)\)

note that *saturation* means that the neuron has *low* or *high* activations (close to 0 or 1) and thus its output is not changing despite varying inputs. i.e. it is not learning.

** pseudocode

*** backprop

1. *Input* x: Set the corresponding activation \(a^1\) for the input layer
2. *Feedforward*: For each \(l=2,3,\ldots,L\) compute \(z^l = w^la^{l-1}+b^l\) and \(a^l = \sigma(z_l)\).
3. *Output Error* \(\delta^l\): Compute the vector \(\delta^L = \nabla_a C \odot \sigma'(z^L)\)
4. *Backpropagate the error:* For each \(l=L-1,L-2,\ldots,2\) compute \(\delta^l = ((w^{l^1})^T \delta^{l^1}) \odot \sigma'(z^l)\)
5. *Output*: The gradient of the cost function is given by \(\cfrac{\partial C}{\partial w_{jk}^l = a_k^{l-1}\delta^l_j}\) and \(\cfrac{\partial C}{\partial b_j^l} = \delta_j^l\).

*** TODO sgd


** speed of backprop

- the total cost of backpropagation is roughly the same as making just two forward passes through the network.
- speedup was first appreciated in 1986, and greatly expanded the range of problems that neural networks could solve.
- as such it is possible to use backprop to train *deep neural nets*.



** fig code                                                        :noexport:

(add-to-list 'org-latex-packages-alist '("outline" "contour" t))
(shell-command "which inkscape")

\begin{tikzpicture}[x=2.7cm,y=1.6cm]
  % Define colors
  \colorlet{myred}{red!80!black}
  \colorlet{myblue}{blue!80!black}
  \colorlet{mygreen}{green!60!black}
  \colorlet{myorange}{orange!70!red!60!black}
  \colorlet{mydarkred}{red!30!black}
  \colorlet{mydarkblue}{blue!40!black}
  \colorlet{mydarkgreen}{green!30!black}
  
  % Define TikZ styles
  \tikzset{
    >=latex, % for default LaTeX arrow head
    node/.style={thick,circle,draw=myblue,minimum size=22,inner sep=0.5,outer sep=0.6},
    node in/.style={node,green!20!black,draw=mygreen!30!black,fill=mygreen!25},
    node hidden/.style={node,blue!20!black,draw=myblue!30!black,fill=myblue!20},
    node convol/.style={node,orange!20!black,draw=myorange!30!black,fill=myorange!20},
    node out/.style={node,red!20!black,draw=myred!30!black,fill=myred!20},
    connect/.style={thick,mydarkblue}, %,line cap=round
    connect arrow/.style={-{Latex[length=4,width=3.5]},thick,mydarkblue,shorten <=0.5,shorten >=1},
    node 1/.style={node in}, % node styles, numbered for easy mapping with \nstyle
    node 2/.style={node hidden},
    node 3/.style={node out}
  }
  
  \message{^^JNeural network activation}
  \def\NI{5} % number of nodes in input layers
  \def\NO{4} % number of nodes in output layers
  \def\yshift{0.4} % shift last node for dots
  
  % INPUT LAYER
  \foreach \i [evaluate={\c=int(\i==\NI); \y=\NI/2-\i-\c*\yshift; \index=(\i<\NI?int(\i):"n");}]
              in {1,...,\NI}{ % loop over nodes
    \node[node in,outer sep=0.6] (NI-\i) at (0,\y) {$a_{\index}^{(0)}$};
  }
  
  % OUTPUT LAYER
  \foreach \i [evaluate={\c=int(\i==\NO); \y=\NO/2-\i-\c*\yshift; \index=(\i<\NO?int(\i):"m");}]
    in {\NO,...,1}{ % loop over nodes
    \ifnum\i=1 % high-lighted node
      \node[node hidden]
        (NO-\i) at (1,\y) {$a_{\index}^{(1)}$};
      \foreach \j [evaluate={\index=(\j<\NI?int(\j):"n");}] in {1,...,\NI}{ % loop over nodes in previous layer
        \draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
        \draw[connect] (NI-\j) -- (NO-\i)
          node[pos=0.50] {\contour{white}{$w_{1,\index}$}};
      }
    \else % other light-colored nodes
      \node[node,blue!20!black!80,draw=myblue!20,fill=myblue!5]
        (NO-\i) at (1,\y) {$a_{\index}^{(1)}$};
      \foreach \j in {1,...,\NI}{ % loop over nodes in previous layer
        %\draw[connect,white,line width=1.2] (NI-\j) -- (NO-\i);
        \draw[connect,myblue!20] (NI-\j) -- (NO-\i);
      }
    \fi
  }
  
  % DOTS
  \path (NI-\NI) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
  \path (NO-\NO) --++ (0,1+\yshift) node[midway,scale=1.2] {$\vdots$};
  
  % EQUATIONS
  \def\agr#1{{\color{mydarkgreen}a_{#1}^{(0)}}} % green a_i^j
  \node[below=16,right=11,mydarkblue,scale=0.95] at (NO-1)
    {$\begin{aligned} %\underset{\text{bias}}{b_1}
       &= \color{mydarkred}\sigma\left( \color{black}
            w_{1,1}\agr{1} + w_{1,2}\agr{2} + \ldots + w_{1,n}\agr{n} + b_1^{(0)}
          \color{mydarkred}\right)\\
       &= \color{mydarkred}\sigma\left( \color{black}
            \sum_{i=1}^{n} w_{1,i}\agr{i} + b_1^{(0)}
           \color{mydarkred}\right)
     \end{aligned}$};
  \node[right,scale=0.9] at (1.3,-1.3)
    {$\begin{aligned}
      {\color{mydarkblue}
      \begin{pmatrix}
        a_{1}^{(1)} \\[0.3em]
        a_{2}^{(1)} \\
        \vdots \\
        a_{m}^{(1)}
      \end{pmatrix}}
      &=
      \color{mydarkred}\sigma\left[ \color{black}
      \begin{pmatrix}
        w_{1,1} & w_{1,2} & \ldots & w_{1,n} \\
        w_{2,1} & w_{2,2} & \ldots & w_{2,n} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        w_{m,1} & w_{m,2} & \ldots & w_{m,n}
      \end{pmatrix}
      {\color{mydarkgreen}
      \begin{pmatrix}
        a_{1}^{(0)} \\[0.3em]
        a_{2}^{(0)} \\
        \vdots \\
        a_{n}^{(0)}
      \end{pmatrix}}
      +
      \begin{pmatrix}
        b_{1}^{(0)} \\[0.3em]
        b_{2}^{(0)} \\
        \vdots \\
        b_{m}^{(0)}
      \end{pmatrix}
      \color{mydarkred}\right]\\[0.5em]
      {\color{mydarkblue}\mathbf{a}^{(1)}} % vector (bold)
      &= \color{mydarkred}\sigma\left( \color{black}
           \mathbf{W}^{(0)} {\color{mydarkgreen}\mathbf{a}^{(0)}}+\mathbf{b}^{(0)}
         \color{mydarkred}\right)
    \end{aligned}$};
  
\end{tikzpicture}


*** colorbox and empheq control

\(\require{color}\colorbox{yellow}{$\sigma$}\)
\(\require{empheq}\)
\(\newcommand{\boxedeq}[1]{\begin{empheq}[box={\fboxsep=6pt\fbox}]{align*}#1\end{empheq}}\)
\(\newcommand{\coloredeq}[1]{\begin{empheq}[box=\colorbox{red}]{align*}#1\end{empheq}}\)

\[\colorbox{lightblue}{$\begin{align}
\delta^L &= \nabla_a C \odot \sigma'(z^L) \label{eq:bp1}\\
\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \label{eq:bp2}\\
\cfrac{\partial C}{\partial b_j^l} &= \delta_j^l \label{eq:bp3}\\
\cfrac{\partial C}{\partial w_{jk}^l} &= a_k^{l-1} \delta_j^l \label{eq:bp4}\\
\end{align}$}\]

**** numbered, bad align. can change gather to align.

\[\fcolorbox{red}{lightblue}{$\begin{gather}
\delta^L &= \nabla_a C \odot \sigma'(z^L)\label{eq:bp1}\\
\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)\label{eq:bp2}\\
\cfrac{\partial C}{\partial b_j^l} &= \delta_j^l\label{eq:bp3}\\
\cfrac{\partial C}{\partial w_{jk}^l} &= a_k^{l-1} \delta_j^l\label{eq:bp4}
\end{gather}$}\]

like so:

\[\fcolorbox{red}{lightblue}{$\begin{align}
\delta^L &= \nabla_a C \odot \sigma'(z^L)\\
\delta^l &= ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)\\
\cfrac{\partial C}{\partial b_j^l} &= \delta_j^l\\
\cfrac{\partial C}{\partial w_{jk}^l} &= a_k^{l-1} \delta_j^l
\end{align}$}\]


