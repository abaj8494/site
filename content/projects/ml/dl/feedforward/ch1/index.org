+++
title = "chapter 1: using neural networks to recognise handwritten digits"
math = "true"
toc = "true"
+++

** notes
- insight is forever
- his code is written in python 2.7
- emotional commitment is a key to achieving mastery
  
#+BEGIN_CENTER
#+CAPTION: The visual cortex is located in the occipital lobe
[[{{< cwd >}}primary-visual.png]]
#+END_CENTER

- primary visual cortex has 140 million neurons
- two types of artificial neuron: perceptron, sigmoid neuron
- perceptron takes binary inputs and produces a single binary output.
- perceptrons should be considered as making decisions after weighing up evidence (inputs)
- neural nets can express =NAND=, which means /any computation/ can be built using these gates!

#+BEGIN_CENTER
#+ATTR_HTML: :id nand :class lateximage :width 300px
[[{{< cwd >}}nand.svg]]
#+END_CENTER

*** sigmoid neurons
- you want to tweak the weights and biases such that small changes in either will produce a small change in the output
- as such we must break free from the sgn step function and introduce the sigmoid function

#+BEGIN_CENTER
#+HTML: <div style="display: flex; align-items: center; justify-content: center; gap: 20px;">
#+HTML:   <div style="text-align: center; display: flex; flex-direction: column; align-items: center;">
#+CAPTION: Binary, Discontinuous Sign
#+ATTR_HTML: :width 300px :class lateximage
[[{{< cwd >}}sgn.svg]]
#+HTML:   </div>
#+HTML:   <div style="font-size: 2em; display: flex; align-items: center;">\(\leadsto\)</div>
#+HTML:   <div style="text-align: center; display: flex; flex-direction: column; align-items: center;">
#+CAPTION: Continuous, Differentiable Sigmoid
#+ATTR_HTML: :width 300px :class lateximage
[[{{< cwd >}}sig.svg]]
#+HTML:   </div>
#+HTML: </div>
#+END_CENTER

thus the mathematics of \(\varphi\) becomes:
\[\begin{align*}
\sigma(z) &\equiv \cfrac{1}{1+e^{-z}}\\
&\implies \cfrac{1}{1+\text{exp}(-\sum_j w_jx_j -b)}
\end{align*}\]

and since the output = \(\sigma(w\cdot x + b)\)
\[\Delta \text{output} \approx \sum_j \frac{\partial \text{output}}{\partial w_j}\Delta w_j + \frac{\partial \text{output}}{\partial b}\Delta b \]
where \(\Delta\) is *change*, not the gradient / nabla!

- the input ound output layers of a neural network are straight-forward, there is an art to the design of the hidden layers.
- /feedforward/ neural networks means there are no loops in the network.
  - [[https://abaj.ai/projects/ml/dl/rnn][recurrent neural networks]] have loops

** gradient descent
- the [[https://abaj.ai/tags/mnist][mnist]] training dataset was composed from 250 people, half of whom were US Census Bureau employees, and the other half of whom were high school students.
- the testing data was taken from a 250 different people part of the same institutions
- we minimise the loss function: \[C(w,b) \equiv \frac{1}{2n}\sum_x \|y(x) -a \|^2 \]
  - a good question to ask is why do we minimise this auxillary function and not just the number of misclassified examples?
  - the answer to this is that the _number of misclassified examples_ is not a continuous value.

\[\begin{align*}
\Delta C &\approx \nabla C \cdot \Delta v\\
\nabla C &\equiv \left( \cfrac{\partial C}{\partial v_1}, \ldots, \cfrac{\partial C}{\partial v_m} \right ) ^T \\
\Delta v &= -\eta \nabla C \\
v&\rightarrow v' = v - \eta\nabla C
\end{align*}\]

then replacing \(v_j\) with \(w_j\) and \(b_l\) yields *gradient descent*

\[\begin{align*}
w_k \rightarrow w_k' &= w_k - \eta\cfrac{\partial C}{\partial w_k}\\
b_l \rightarrow b_l' &= b_l - \eta\cfrac{\partial C}{\partial b_l}
\end{align*}\]

however, clearly for *huge* datasets, taking the partial derivatives w.r.t every weight and bias would be computationally infeasible.

thus we average over a smaller number of batches, called mini-batches:

\[\begin{align*}
\nabla C &= \cfrac{\sum_x \nabla C_x}{n} \\
&\approx \cfrac{\sum^m_{j=1}\nabla C_{x_j}}{m}
\end{align*}\]

and then use those in the gradient calculation instead:

\[\begin{align*}
w_k \rightarrow w_k' &= w_k - \frac{\eta}{m}\sum_j\cfrac{\partial C_{x_j}}{\partial w_k}\\
b_l \rightarrow b_l' &= b_l - \frac{\eta}{m}\sum_j\cfrac{\partial C_{x_j}}{\partial b_l}
\end{align*}\]

* implementation

#+INCLUDE: "/code/10khrs-ai-ml-dl/learning/nielsen-dl/ch1-network.py" src python

#+INCLUDE: "/code/10khrs-ai-ml-dl/learning/nielsen-dl/mnist_loader.py" src python


* fig                                                              :noexport:
** arbitrary neuron 

\begin{tikzpicture}
    \tikzset{
        inputNode/.style={draw,circle,minimum size=10pt,inner sep=0pt},
        stateTransition/.style={->, thick}
    }
    
	\node[draw,circle,minimum size=25pt,inner sep=0pt] (x) at (0,0) {$\Sigma$ $\varphi$};

	\node[inputNode] (x0) at (-2, 1.5) {\color{red}$\tiny x_0$};
	\node[inputNode] (x1) at (-2, 0.75) {\color{red}$\tiny x_1$};
	\node[inputNode] (x2) at (-2, 0) {\color{red}$\tiny x_2$};
	\node[inputNode] (x3) at (-2, -0.75) {\color{red}$\tiny x_3$};
	\node[inputNode] (xn) at (-2, -1.75) {\color{red}$\tiny x_n$};

	\draw[stateTransition] (x0) to[out=0,in=120] node [midway, sloped, above=-2] {\color{green!40!black}$w_0$} (x);
	\draw[stateTransition] (x1) to[out=0,in=150] node [midway, sloped, above=-2] {\color{green!40!black}$w_1$} (x);
	\draw[stateTransition] (x2) to[out=0,in=180] node [midway, sloped, above=-2] {\color{green!40!black}$w_2$} (x);
	\draw[stateTransition] (x3) to[out=0,in=210] node [midway, sloped, above=-2] {\color{green!40!black}$w_3$} (x);
	\draw[stateTransition] (xn) to[out=0,in=240] node [midway, sloped, above=-2] {\color{green!40!black}$w_n$} (x);
    \color{blue}
	\draw[stateTransition] (x) -- (2,0) node [midway,above=-0.1cm] {};
	\draw[dashed] (0,-0.43) -- (0,0.43);
	\node (dots) at (-2, -1.15) {$\vdots$};
\end{tikzpicture}

** sgn plot

\begin{tikzpicture}
    \begin{axis}[
        legend pos=north west,
        legend style={
            font=\small, 
            text width=4cm, 
            cells={anchor=north},
            legend cell align=left,
            minimum height=2cm,
            inner ysep=5pt
        },
        axis x line=middle,
        axis y line=middle,
        y tick label style={/pgf/number format/fixed,
                            /pgf/number format/fixed zerofill,
                            /pgf/number format/precision=1},
        grid = major,
        width=16cm,
        height=8cm,
        grid style={dashed, gray!30},
        xmin=-8,     % start the diagram at this x-coordinate
        xmax= 8,     % end   the diagram at this x-coordinate
        ymin= -1,    % start the diagram at this y-coordinate
        ymax= 1,     % end   the diagram at this y-coordinate
        %axis background/.style={fill=white},
        xlabel=$t$,
        ylabel=sgn$(t)$,
        tick align=outside,
        legend image post style={scale=0.5},  % Makes the legend line smaller
        enlargelimits=true]
      % plot the sign function
      \addplot[domain=-8:8, blue, ultra thick,samples=500] {ifthenelse(x<0,-1,ifthenelse(x>0,1,0))};
      \addlegendentry{sgn$(t)=\begin{cases}
        -1 & \text{if } t < 0 \\
        0 & \text{if } t = 0 \\
        1 & \text{if } t > 0
      \end{cases}$}
    \end{axis}
\end{tikzpicture}

** sigmoid plot

 \begin{tikzpicture}
    \begin{axis}[
        legend pos=north west,
        axis x line=middle,
        axis y line=middle,
        y tick label style={/pgf/number format/fixed,
                            /pgf/number format/fixed zerofill,
                            /pgf/number format/precision=1},
        grid = major,
        width=16cm,
        height=8cm,
        grid style={dashed, gray!30},
        xmin=-8,     % start the diagram at this x-coordinate
        xmax= 8,     % end   the diagram at this x-coordinate
        ymin= 0,     % start the diagram at this y-coordinate
        ymax= 1,     % end   the diagram at this y-coordinate
        %axis background/.style={fill=white},
        xlabel=$t$,
        ylabel=sig$(t)$,
        tick align=outside,
        enlargelimits=true]
      % plot the stirling-formulae
      \addplot[domain=-8:8, red, ultra thick,samples=500] {1/(1+exp(-1*x))};
      \addlegendentry{sig$(t)=\frac{1}{1+e^{-t}}$}
    \end{axis}
\end{tikzpicture}

** nand fig

\begin{tikzpicture}
\node[draw] (x1) at (0,2) {$x_1$};
\node[draw] (x2) at (0,0) {$x_2$};
\node[draw, circle] (n) at (2,1) {3};
\draw[->] (x1) -- (n) node[midway, above] {-2};
\draw[->] (x2) -- (n) node[midway, above] {-2};
\draw[->] (n) -- ++(2,0);
\end{tikzpicture}


