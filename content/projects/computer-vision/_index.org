+++
title = "Computer Vision"
math = "true"
toc = "true"
tikzjax = "true"
+++

I think taking a course in a subject that you are interested in is never particularly a bad thing.

This page will exclusively contain what I was taught in COMP9517 for a short while. That is just how it will be.

However, eventually we will morph out of this into the textbook, and subsequently out of that too.

For now, I just want a document to type in that I enjoy typing in!

* Notes

in each of the below topics, there are a number of algorithms and facts that I must know for the final exam.

** Image Formation

- pinhole camera model
- projective geometry
  - vanishing lines and points
  - mathematics: \[x' = -x \frac{f}{z} \] \[ y' = -y \frac{f}{z} \]
- colour spaces:
  - RGB: red green blue
    - strongly correlated channels
  - HSV: hue saturation value
    - confounded channels
  - YCbCr: stand for ??
    - fast to compute, good for compression. the modern day standard
    - Y = luminance
    - Cb = blue colour difference
    - Cr = red colour difference
  - L*a*b*: differences in luminance are more perceptually uniform
    - L* = /lightness/
    - 
- quantisation: digitises the image intensity / amplitude values


** Image Processing

*** spatial domain

**** point operations

\[T: \mathbb{R} \rightarrow \mathbb{R}\quad g(x,y) =T(f(x,y))\]

- contrast stretching
- intensity thresholding
  - automatic: otsu, isodata, multilevel
- intensity inversion
- log transformation
- power transformation
- piecewise linear transformation
- piecewise contrast stretching
- gray-level slicing
- bit-plane slicing
- histogram of pixel values
- histogram-based thresholding "triangle"
- histogram equalisation
  - continuous; discrete
  - constrained
- histogram matching
  - continuous, discrete
- arithmetic and logical operations
- averaging

**** neighourhood operations

\[T: \mathbb{R^n} \rightarrow \mathbb{R}\quad g(x,y) = T(f(x,y),f(x+1,y),f(x-1,y),...\]

- convolution
  - linear, shift-invariant
  - properties:
    - commutativity \[f_1 * f_2 = f_2 * f_1 \]
    - associativity \[f_1 * (f_2 * f_3) = (f_1 * f_2) * f_3 \]
    - distributivity \[f_1 * (f_2 * f_3) = f_1 * f_2 + f_1 * f_3\]
    - multiplicativity \[a(f_1*f_2) = (a f_1) * f_2 = f_1 * (a f_2) \]
    - derivation \[(f_1 * f_2)' = f_1'*f_2 = f_1*f_2' \]
    - theorem \[f_1 * f_2 \iff \hat{f_1} \hat{f_2}\] convolution in spatial domain amounts to multiplication in spectral domain
- spatial filtering
- linear shift-invariant operations
- border problem
  - padding: add more pixels with value 0
  - clamping: repeat all border pixel values indefinitely
  - wrapping: copy pixel values from opposite sides
  - mirroring: reflect pixel values across borders

***** filtering methods

- uniform filter
  - smoothing
- gaussian filter
  - separable and circularly symmetric; the only such filter
  - optimal joint localisation in spatial and frequency domain
  - fourier transform (ft henceforth) is also a Gaussian
  - n-fold convolution of any low-pass filter converges to a Gaussian
  - infinitely smooth, so infinite derivatives exist
  - good at keeping small objects (better than median). it is a smoothing filter.
- median filter
  - order-statistic filter
  - sorts, then takes median
  - can *eliminate salt and pepper noise* (which are just isolated intensity spikes)
  - nonlinear filter
  - better than gaussian at removing small objects
- smoothing
  - image blurring, noise reduction
- differentiation
  - forward, backward, central difference (finite differences because images are discrete)
- separability
  - improves computation efficiency
  - examples: uniform, prewitt, sobel, gauss
- pooling
  - max / min / average
  - makes image smaller
  - combines filtering and downsampling in one operation

***** image enhancement

- sharpening
  - subtract Gaussian filtered from image, then add the produced "high-frequencies" back into the image.
  - can also use the laplacean: \(\nabla^2 f = f_{xx} + f_{yy} \) by subtracting it from the original image: \[f(x,y) - \nabla^2 f(x,y) \]
- unsharp masking
  - ?
- gradient vector & magnitude
  - \[\nabla f(x,y) = [f_x(x,y), f_y(x,y)]^T \]
  - \[||\nabla f(x,y) || = \sqrt{f_x^2(x,y),f_y^2(x,y)} \]
- edge detection
  - use laplacean or intensity gradient

*** transform domain

{{< tikz >}}
\begin{tikzpicture}[%
        >=stealth,                                % arrow style
        box/.style={draw,rounded corners=4pt,     % common style for blocks
                    fill=gray!20,
                    minimum width=4cm,
                    minimum height=1.6cm,
                    align=center},
        node distance=2.7cm                       % horizontal spacing
    ]

  % --- nodes ---
  \node[coordinate] (in) {};                     % entry point
  \node[box,right=of in]     (fourier) {Fourier\\ transform};
  \node[box,right=of fourier](filter)  {Frequency\\ filtering};
  \node[box,right=of filter] (inv)     {Inverse Fourier\\ transform};
  \node[coordinate,right=of inv] (out) {};       % exit point

  % --- arrows & labels ---
  \draw[->] (in)     -- (fourier) node[midway,below=4pt] {$f(x,y)$};
  \draw[->] (fourier) -- (filter) node[midway,below=4pt] {$F(u,v)$};
  \draw[->] (filter)  -- (inv)    node[midway,below=4pt] {$F(u,v)H(u,v)$};
  \draw[->] (inv)     -- (out)    node[midway,below=4pt] {$g(x,y)$};

\end{tikzpicture}
{{< /tikz >}}

- high frequency -> rapidly changing intensities across pixels
- low frequency -> large scale image structures
- we process images in the frequency domain by first applying the Fourier transform

**** Fourier Transform
:PROPERTIES:
:CUSTOM_ID: fourier-transform
:END:

- interpretations:
  - frequencies correspond to patterns
  - $F(0,0)$ is the total intensity over all pixels of the image
  - noise (typically) corresponds to fluctuations in the highest frequencies
  - 

- notation:
  - $f(x)$ is the spatial input function
  - $F(u)$ is the Fourier transform
  - $e^{i\omega x} = \cos(\omega x) + i\sin(\omega x) $
  - $\omega = 2\pi u$ is radial frequency
  - $u$ is spatial frequency

- forward fourier transform \[F(u) = \int^\infty_{-\infty} f(x)\; e^{\displaystyle -i 2\pi u x}\,\mathrm{d}x\]
- inverse fourier transform \[f(x) = \int^\infty_{-\infty} F(u)\; e^{\displaystyle i 2\pi u x}\,\mathrm{d}u\]

- properties:
| Property        | Spatial             | Frequency                   |
|-----------------+---------------------+-----------------------------|
| Superposition   | $f_1(x) + f_2(x)$   | $F_1(u) + F_2(u)$           |
| Translation     | $f(x-\Delta x)$     | $F(u)e^{-i 2\pi u\Delta x}$ |
| Convolution     | $f(x)*h(x)$         | $F(u)H(u)$                  |
| Correlation     | $f(x) \otimes h(x)$ | $F(u)H^*(u)$                |
| Multiplication  | $f(x)h(x)$          | $F(u)*H(u)$                 |
| Scaling         | $f(ax)$             | $F(u/a)/a$                  |
| Differentiation | $f^{(n)}(x)$        | $(i2\pi u)^n F(u)$          |

- 2D:
  - forward fourier transform \[F(u,v) = \int^\infty_{-\infty}\int^\infty_{-\infty} f(x,y)\; e^{\displaystyle -i 2\pi (ux+vy)}\;\mathrm{d}x\,\mathrm{d}y\]
  - inverse fourier transform \[f(x,y) = \int^\infty_{-\infty}\int^\infty_{-\infty} F(u,v)\; e^{\displaystyle -i 2\pi (ux+vy)}\;\mathrm{d}u\,\mathrm{d}v\]
  - $f \leftrightarrow F$: fourier transform pair
  - $F = R + i I$: real plus imaginary part
  - $|F| = \sqrt{R^2 + I^2}$: Magnitude
  - $\phi = \arctan(\frac{I}{R})$: Phase

- Discrete:
  - forward \[F(u,v) = \sum_{x=0}^{M-1} \sum_{y=0}^{N-1} f(x,y)\;e^{\displaystyle -i 2 \pi (\frac{ux}{M} + \frac{vy}{N})} \] for $u=0... M-1$ and $v = 0... N -1$
  - inverse \[f(x,y) = \frac{1}{MN} \sum_{u=0}^{M-1} \sum_{v=0}^{N-1} F(u,v)\;e^{\displaystyle i 2\pi (\frac{ux}{M} + \frac{vy}{N})} \] for $x=0... M-1$ and $y = 0... N -1$

**** filtering

- procedure:
  1. multiply input image $f(x,y)$ by $(-1)^{x+y}$ to ensure centering $F(u,v)$
  2. compute the transform $F(u,v)$ from image $f(x,y)$ using 2D DFT
  3. multiply $F(u,v)$ by a centred filter $H(u,v)$ to obtain result $G(u,v)$
  4. compute the inverse 2D DFT of $G(u,v)$ to obtain the spatial result $g(x,y)$
  5. take the real component of $g(x,y)$ (imaginary component is zero)
  6. multiply the result by $(-1)^{x+y}$ to remove the pattern introduced in step 1^^

***** convolution theorem (how does this relate to convolution?)

- filtering in the frequency domain can be computationally more efficient
- more intuitive in freq dom. i.e:
  - low-pass = keep low frequencies, but attenuate {{< mnote "reduce the effect of" >}} high frequencies
  - high-pass = keep high freq, reduce low freq
  - band-pass = keep frequencies /in a given band/. attenuate the rest
  - take inverse to get the corresponding spatial filter


- notch filtering = opposite of band-pass; attenuates a given range.

- difference of Gaussians is a high-pass
- gaussian filter = low-pass
- image pyramids is for multi-resolution
- approximation = ?
- reconstruction = ?


** Feature Representation

- image features are *vectors* that are a compact representation of images. i.e. blobs, edges, corners, etc.
- more efficient and robust way to represent images. also useful for further processing: object detection, image segmentation, classification, retrieval, stitching and object tracking.
- note that pixel values are _highly redundant_ and victim to light intensity, colour, angle changes, camera orientation
- we wish for features to be: reproducible, salient and compact (aka robust, descriptive, efficient)

*** colour features

- colour is easy to compute
- invariant to image scaling, translation, rotation
  
**** colour histogram

(TODO add image demo?)
- represent the global distribution of pixel colours in an image
- step 1: construct a histogram for each colour channel (R, G, B)
- step 2: concatenate the histogrames (vectors) of all channels as the final feature vector.
  
**** colour moments

- moments based representation of colour distributions
- gives a feature vector of only 9 elements (for RGB)
- lower representation capability than above histogram

*** texture features

- visual characteristics and appearance of objects
- a powerful discriminating feature for identifying visual patterns
- encodes properties of structural homogeneity beyond colour or intensity

**** haralick features

- array of statistical descriptors of image patterns
- captures spatial relationship between neighbouring pixels
- step 1: construct the gray-level co-occurence matrix (GLCM) - representing the frequency of peixel intensity pairs occurring at a specific offset and direction
- step 2: compute the Haralick feature descriptors from the GLCM - that summarises texture information (how pixel intensities are spatially related)
- often used in practice due to their simplicity and interpretability.
  
***** glcm method

- TODO not sure of best explanation

***** haralick descriptors

- TODO seems tacky and long.

**** local binary patterns

- describe the spatial structure of local image texture
algorithm:
  - divide the image into cells of $N\times N$ pixels ($N=16$ or $N=32$)
  - compare each pixel in a given cell to each of its 8 neighbouring pixels
  - if the neighbour's value is *greater than or equal to* the centre, write 1; otherwise write 0.
  - this gives an 8-digit binary pattern per pixel, representing a value in the range $0...255$
  - count the number of times each 8-digit binary number occurs in the cell
  - this gives a 256-bin histogram (also known as the LBP feature vector)
  - combine the histograms of all cells of the given image
  - this gives the image-level LBP feature descriptor

TODO insert example

- LBP can be multiresolution and rotation-invariant
  - in the case of multiresolution, you vary the distance between the centre pixel and neighbouring pixels and vary the number of neighbouring pixels
  - for rotation-invariance: vary the way of constructing the 8-digit binary number by performing bitwise shift to derive the smallest number
    - note: not all patterns have 8-shifted variants (i.e. 11001100 has only 4)
    - reduces LPB feature dimension from 256 to 36

**** scale-invariant feature transform

- describes texture in a localised region around a *keypoint*
- invariant to scaling, rotation, shift.
- robust to affine distortion and illumination changes

***** algorithm
- Scale-Space Extrema Detection: find maxima/minima in DoG images across scales
- Keypoint Localisation: discard low-contrast keypoints and eliminate edge responses
- Orientation Assignment: achieve rotation invariance by orientation assignment (make histogram of local gradient vectors) {{< mnote "i forgot this step in the lab" >}}
- Keypoint Descriptor: compute gradient orientation histograms

***** descriptor matching
nearest neighbour distance ratio (NNDR) \[NNDR = \frac{d_1}{d_2} = \frac{||D_a -D_B||}{||D_A-D_C||}\]
  - distance $d_1$ is to the first nearest neighbour
  - distance $d_2$ is to the second nearest neighbour
  - nearest neighbours in 128D feature space
  - reject matches with NNDR > 0.8

- translation and rotation are *rigid transformations*
- scaling, affine, perspective are *nonrigid transformations*
- TODO optional: add matrices

***** fitting and alignment
- Least Squares (LS) fitting of corresponding keypoints $(x_i,x_i')$
  - find parameters $p$ that minimise the squared error E \[E = \sum_i ||T(\mathbf{x_i); \mathbf{p) - \mathbf{x_i'}||^2\]
- RANdom SAmple Consensus (RANSAC) fitting
  - least-squares is *hampered by outliers*
  - better use a subset of the data and check inlier agreement
  - RANSAC does this in an iterative way to find the optimum
  - algorithm:
    1. sample (randomly) the number of points required to fit the model
    2. solve for the model parameters using the samples
    3. score by the fraction of inliers within a preset threshold of the model
    - repeat 1-3 until the best model is found with high confidence

- TODO practise problem on solving the "transformation" given matched points A and B.

*** feature encoding

- Bag-of-Words (BoW) takes variable number of local image features and encodes them into a fixed-dimensional histogram. it works in general, but for SIFT too.
- Algorithm:
  - extract local SIFT keypoint descriptors from training images
  - create the "vocabulary" from the set of SIFT keypoint descriptors
    - use K-means clustering
      - partitions the training data into $k$ categories
      - algorithm:
	- initialise: $k$ cluster centres (randomly)
	- iterate:
	  1. assign data (feature vectors) to the closest cluster (Euclidean distance)
	  2. update cluster centres as the mean of the data samples in each cluster
	- terminate:
	  - when converged or the number of iterations reaches the maximum
  - this vocabulary represents the categories of local descriptors
  - cluster centres are the "visual words" in this "vocabulary" used to represent an image
  - each local feature descriptor is assigned to one visual word with the smallest distance
  - compute the number of local image feature descriptors assigned to each visual word
  - concatenate the numbers into a vector which is the "BoW" representation of the image

- local features (that BoW takes in) can be LBP, SURF, BRIEF, ORB
  - BoW in turn can be replaced with VLAD, Fisher Vector

*** shape features

- essential characteristic of material objects
- typically extracted after image segmentation
- can be used to identify and classify objects
- challenges
  - invariant to rigid transformations
  - tolerant to non-rigid deformations

**** basic shape features

- net area; principal axes; convex area
- convexity, concavity; convex hull {{< mnote "recall that this is the smallest bounding box around the object that is also convex" >}}; convex deficiency (set difference between the convex hull and the object)
- compactness; circularity
  - ^inversely related
  - compactness: ratio of the area of a circle with the same perimeter as the object to the area of the object
  - circularity: ratio of $4\pi$ times the area of an object to the second power of its perimeter ($4\pi A/p^=1$ for a circle)
- elongation; eccentricity
  - elongation: ratio between the length and width of the object's bounding box
  - eccentricity: ratio of the length of the minor axis to the length of the major axis

**** boundary descriptors (TODO)

- chain code descriptor
- local curvature descriptor
- global curvature descriptors:
  - total bending energy $B=\oint_C K^2\; \mathrm{d}s$
  - total absolute curvature $K=\oint_C |K(s)|\;\mathrm{d}s$
- radial distance descriptor

**** shape context

- is a point-wise local feature descriptor
  - pick $n$ points $p_i$ on the contour of a shape
  - for each point, create a radial coordinate system centred at this point and compute a histogram $h_i$ based on the relative coordinates of the other $n-1$ points
  - this is the shape context of $p_i$
- e.g. shape matching ALGORITHM TODO; TODO graphic

**** histogram of oriented gradients (HoG)

- describes the distributions of gradient orientations in localised areas
- does not require initial segmentation
- algorithm:
  1. calculate the gradient vector at each pixel
     - gradient magnitude
     - gradient orientation
  2. construct the gradient histogram of all pixels in a cell
     - divide orientations into $N$ bins (typically $N=9$ bins evenly splitting 180 degrees)
     - assign the gradient magnitude of each pixel to the bin corresponding to its orientation
  3. generate detection-window level HOG descriptor
     - concatenate cell histograms
     - block-normalise cell histograms
      
sliding window ? TODO

- use-case: detecting humans in images.


** Pattern Recognition

** Image Segmentation

** Deep Learning

** Motion and Tracking

* homework

- explain the transforms based on the slides.
- Image Features: Shape Matching Algorithm; Descriptors
