+++
title = "Running LLM's locally"
tags = ["llm", "mac", "ram"]
+++

* Context:
Running LLM's (large-language-models) locally is now possible [fn:1] due to the abundance of highly parallelised compute (GPU's) at affordable prices and also the advances of Deep Learning in the past decade.

As such, even slightly powerful consumer devices such as my M1 Macbook Pro with 8GB of RAM can run a small LLM. The purpose of this post is to investigate the token speed and accuracy of a variety of LLM's on my Machines.


* Instructions

Courtesy of Alex Ziskind's [[https://www.youtube.com/watch?v=bp2eev21Qfo][tutorial]]:
1. clone the frontend https://github.com/open-webui/open-webui.git
2. install ollama from https://ollama.com
3. in the command line pull whichever model you like: =ollama pull llama3=

* 

* Footnotes

[fn:1]as of 2024 
