+++
title = "Memory"
tags = ["notes", "stack-pointer", "registers", "cache", "L1", "L2", "stack", "heap"]
toc = "true"
+++

Honestly, the diagrams that I wish to reproduce already exist [[https://azeria-labs.com/writing-arm-assembly-part-1/][here]]. Currently this page is in construction and probably will be until I finish my Doctorate.

#+BEGIN_QUOTE
‚ÄúMemory is the mother of all wisdom."
--- Aeschylus
#+END_QUOTE

* Babbage's Big Brain

* Memory as a Hierarchy ‚Äî Not a Monolith
:PROPERTIES:
:CUSTOM_ID: mem-hierarchy
:END:

Hierarchy exists for two intertwined reasons:

1. **Physics** ‚Äì Smaller structures are faster and nearer to ALUs but hold less data; larger structures store more but are farther away and thus slower.
2. **Economics** ‚Äì Fast memory costs disproportionately more per byte.

An efficient system arranges *multiple* layers so that
> the majority of accesses hit the *small, fast* part,
> while the *bulk* of bytes reside in the *large, cheap* part.

* The Register File ‚Äì Your Fastest Scratchpad
Modern x86-64 cores provide 16 ‚Äì 32 architectural registers
(`%rax`, `%rbx`, ‚Ä¶) plus special purpose ones
(program counter **PC/IP**, stack pointer **%rsp**, etc.).
Access is single-cycle and **superscalar** execution renames physical
registers to squeeze out more parallelism.

_Practical take-away_: keep **live data** small and in registers;
compilers often need help via `restrict`, `const`, or `register` hints.

* Caches: The Illusion of Speed

**Locality** (temporal & spatial) is the hinge on which caches swing.

| Level | Typical Size | Line / Block | Hit ‚âà cycles | Miss goes to |
|-------+--------------+--------------+--------------+--------------|
| L1d   | 32 ‚Äì 64 KiB  | 64 B         | 3‚Äì5          | L2           |
| L2    | 256 KiB‚Äì2 MiB| 64 B         | 10‚Äì15        | L3           |
| L3    | 4 ‚Äì 64 MiB (shared) | 64 B | 30‚Äì50        | DRAM         |

Policies  
- **Write-back** keeps writes local until eviction  
- **Replacement** often PLRU or variants, not true LRU  
- **Coherence** (MESI, MOESI) keeps multi-core views in sync

Compiler flags like `-O3 -march=native` align loops, unroll, vectorise,
and prefetch to exploit these caches.

* Main Memory ‚Äì DRAM, Rows and Banks
Dynamic RAM stores bits as charge in capacitors that must be **refreshed**
(~64 ms).  
It is organised into channels ‚Üí DIMMs ‚Üí ranks ‚Üí banks ‚Üí rows ‚Üí columns.
Parallelism across banks hides activate/precharge delays.

üí° **Row buffer hits** are the DRAM analogue of a cache hit.

* Virtual Memory and the Page Table
Every process uses a *flat* 64-bit address space,
translated by hardware page table walkers into physical frames.

- Typical page = 4 KiB; huge pages = 2 MiB or 1 GiB
- **TLB** (Translation Look-aside Buffer) caches recent mappings  
  ‚Äì a miss costs ‚â≥ 30 cycles even before DRAM latency.

`mmap`, `malloc`, stacks and shared libraries are simply
differently-protected regions in that same virtual space.

* Stack vs Heap ‚Äì Two Growth Patterns
| Property          | Stack                      | Heap                         |
|-------------------+----------------------------+------------------------------|
| Lifespan          | Automatic (scope-bound)    | Manual / GC                  |
| Growth direction  | Down in x86-64             | Up (allocator-dependent)     |
| Allocation cost   | 1 instruction (`sub rsp`)  | `malloc` ‚Äì usually `O(1)` average, but slower |
| Typical use-case  | Local variables, return PCs| Dynamic data structures      |

* Anatomy of a Function Call

The **stack pointer** moves just once per call entry/exit.
High-performance code keeps frames shallow to stay inside L1.

* CPU vs GPU Memory ‚Äî Different Beasts
Where CPUs favour *latency*, GPUs are built for *throughput*.

| Aspect              |  CPU                                   |  GPU                                             |
|---------------------+----------------------------------------+--------------------------------------------------|
| Core count          | 8 ‚Äì 64 ‚Äúfat‚Äù cores                     | Hundreds‚Äìthousands of ‚Äúthin‚Äù cores               |
| L1/L2 size per SM   | 32‚Äì64 KiB (data)                       | 64‚Äì128 KiB split **shared mem / cache**          |
| Global memory       | DDR4/5, ~50‚Äì100 GB/s                  | GDDR6/HBM ‚âà 0.5‚Äì1 TB/s                           |
| Access granularity  | 64-byte lines                          | 32-byte **warps**; coalescing crucial            |
| Cache coherence     | Hardware-coherent within socket        | Often manual (CUDA `__syncthreads`, barriers)    |
| Host interaction    | Unified virtual mem (recent)           | PCIe/ NVLink latency, explicit copies still common |

**Key GPU concepts**

*Warp divergence* ‚Äì Threads in a warp that take different branches stall others.  
*Shared memory* ‚Äì Programmer-managed scratchpad; think ‚Äúuser-controlled L1‚Äù.  
*Memory coalescing* ‚Äì Adjacent threads should access adjacent addresses.



